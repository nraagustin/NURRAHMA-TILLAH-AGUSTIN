import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import random

DATASET_PATH = r"C:\Users\Lenovo\OneDrive\Documents\TA\DATASET"
CATEGORIES = ["BERDIRI", "DUDUK", "JATUH"]
FRAME_COUNT = 20
FRAME_SIZE = (64, 64)

def load_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while len(frames) < FRAME_COUNT:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, FRAME_SIZE)
        frame = frame / 255.0
        frames.append(frame)
    cap.release()

    while len(frames) < FRAME_COUNT and len(frames) > 0:
        frames.append(frames[-1])
    return np.array(frames)

def load_dataset(split):
    X, y = [], []
    split_path = os.path.join(DATASET_PATH, split)
    for category in CATEGORIES:
        category_path = os.path.join(split_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: {category_path} tidak ditemukan.")
            continue

        for video_name in os.listdir(category_path):
            video_path = os.path.join(category_path, video_name)
            video_frames = load_video(video_path)
            if video_frames.shape[0] == FRAME_COUNT:
                X.append(video_frames)
                y.append(category)

    print(f"[INFO] Dataset {split}: {Counter(y)}")

    return np.array(X), np.array(y)

X_train, y_train = load_dataset('train')
X_val, y_val = load_dataset('val')

label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_val_enc = label_encoder.transform(y_val)

model = models.Sequential([
    layers.Conv3D(32, (3, 3, 3), 
                  activation='relu', 
                  padding='same', 
                  input_shape=(FRAME_COUNT, FRAME_SIZE[0], FRAME_SIZE[1], 3),
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Dropout(0.3),

    layers.Conv3D(64, (3, 3, 3), 
                  activation='relu', 
                  padding='same',
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Dropout(0.5),

    layers.Flatten(),

    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),

    layers.Dense(len(CATEGORIES), activation='softmax')
])

model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    X_train, y_train_enc,
    validation_data=(X_val, y_val_enc),
    epochs=50,
    batch_size=4,
    verbose=1
)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Model')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

X_test, y_test = load_dataset('test')
y_test_enc = label_encoder.transform(y_test)

y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

cm = confusion_matrix(y_test_enc, y_pred)
report = classification_report(y_test_enc, y_pred, target_names=label_encoder.classes_)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Test Data)')
plt.tight_layout()
plt.show()

print("Classification Report (Test Data):\n", report)

model.save('3model1.h5')
print("Model saved successfully to 3model4.h5")

import json

model_config = {
    'categories': CATEGORIES,
    'frame_count': FRAME_COUNT,
    'frame_size': FRAME_SIZE,
    'epochs_trained': len(history.history['accuracy']),
    'final_training_accuracy': history.history['accuracy'][-1],
    'final_validation_accuracy': history.history['val_accuracy'][-1]
}

with open('3model1.json', 'w') as f:
    json.dump(model_config, f, indent=4)

print("Model metadata saved to 3model.json")
