# Import library 
import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import random

# Dataset path
DATASET_PATH = r"C:\Users\Lenovo\OneDrive\Documents\TA\DATASET"
CATEGORIES = ["BERDIRI", "DUDUK", "JATUH"]
FRAME_COUNT = 20
FRAME_SIZE = (64, 64)

# Load video function
def load_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while len(frames) < FRAME_COUNT:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, FRAME_SIZE)
        frame = frame / 255.0
        frames.append(frame)
    cap.release()

    # Pad with last frame if less than FRAME_COUNT
    while len(frames) < FRAME_COUNT and len(frames) > 0:
        frames.append(frames[-1])
    return np.array(frames)

# Load dataset function
def load_dataset(split):
    X, y = [], []
    split_path = os.path.join(DATASET_PATH, split)
    for category in CATEGORIES:
        category_path = os.path.join(split_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: {category_path} tidak ditemukan.")
            continue

        for video_name in os.listdir(category_path):
            video_path = os.path.join(category_path, video_name)
            video_frames = load_video(video_path)
            if video_frames.shape[0] == FRAME_COUNT:
                X.append(video_frames)
                y.append(category)

    print(f"[INFO] Dataset {split}: {Counter(y)}")

    return np.array(X), np.array(y)

# Add data augmentation function
def apply_data_augmentation(X, y):
    X_aug, y_aug = [], []
    
    # Keep original data
    X_aug.extend(X)
    y_aug.extend(y)
    
    # Apply random augmentations to each video
    for i in range(len(X)):
        video = X[i]
        label = y[i]
        
        # Random brightness adjustment
        if random.random() > 0.5:
            brightness_factor = random.uniform(0.7, 1.3)
            augmented_video = np.clip(video * brightness_factor, 0, 1)
            X_aug.append(augmented_video)
            y_aug.append(label)
        
        # Random horizontal flip
        if random.random() > 0.5:
            flipped_video = np.flip(video, axis=2)
            X_aug.append(flipped_video)
            y_aug.append(label)
        
        # Random rotation (slight)
        if random.random() > 0.5:
            augmented_video = []
            for frame in video:
                rows, cols, _ = frame.shape
                angle = random.uniform(-15, 15)
                M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
                rotated_frame = cv2.warpAffine(frame, M, (cols, rows))
                augmented_video.append(rotated_frame)
            X_aug.append(np.array(augmented_video))
            y_aug.append(label)
            
        # Add extra augmentation for JATUH class to improve its recognition
        if label == "JATUH":
            # Combination of slight rotation and brightness
            if random.random() > 0.5:
                augmented_video = []
                brightness_factor = random.uniform(0.8, 1.2)
                for frame in video:
                    rows, cols, _ = frame.shape
                    angle = random.uniform(-10, 10)
                    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
                    rotated_frame = cv2.warpAffine(frame, M, (cols, rows))
                    bright_frame = np.clip(rotated_frame * brightness_factor, 0, 1)
                    augmented_video.append(bright_frame)
                X_aug.append(np.array(augmented_video))
                y_aug.append(label)
    
    return np.array(X_aug), np.array(y_aug)

# Example load dataset
X_train, y_train = load_dataset('train')
X_val, y_val = load_dataset('val')

# Apply data augmentation to training data only
X_train, y_train = apply_data_augmentation(X_train, y_train)
print(f"[INFO] Dataset setelah augmentasi: {X_train.shape}")

# Label encoding
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_val_enc = label_encoder.transform(y_val)

# One-hot encode the labels for categorical_crossentropy
y_train_onehot = tf.keras.utils.to_categorical(y_train_enc, num_classes=len(CATEGORIES))
y_val_onehot = tf.keras.utils.to_categorical(y_val_enc, num_classes=len(CATEGORIES))

# Model definition with the second block removed
model = models.Sequential([
    # First convolutional block
    layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same', 
                  input_shape=(FRAME_COUNT, FRAME_SIZE[0], FRAME_SIZE[1], 3),
                  kernel_regularizer=regularizers.l2(0.0005)),
    layers.BatchNormalization(),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Dropout(0.3),

    # Third convolutional block (now becomes second block)
    layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same',
                  kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Dropout(0.5),

    layers.Flatten(),
    layers.Dense(256, activation='relu', 
                kernel_regularizer=regularizers.l2(0.002),
                activity_regularizer=regularizers.l1(0.0003)),
    layers.Dropout(0.5),
    layers.Dense(len(CATEGORIES), activation='softmax')
])

# Compile model
model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Train model
history = model.fit(
    X_train, y_train_onehot,
    validation_data=(X_val, y_val_onehot),
    epochs=50,
    batch_size=4,
    verbose=1
)

# Plot akurasi & loss
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Model')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# === Confusion Matrix ===
# Prediksi pada data validasi
y_pred_probs = model.predict(X_val)
y_pred = np.argmax(y_pred_probs, axis=1)

# Confusion matrix dan classification report
cm = confusion_matrix(y_val_enc, y_pred)
report = classification_report(y_val_enc, y_pred, target_names=label_encoder.classes_)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()

# Tampilkan classification report
print("Classification Report:\n", report)

# Evaluate on test set (if available)
try:
    X_test, y_test = load_dataset('test')
    y_test_enc = label_encoder.transform(y_test)
    y_test_onehot = tf.keras.utils.to_categorical(y_test_enc, num_classes=len(CATEGORIES))
    
    test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)
    print(f"\nTest accuracy: {test_acc:.4f}")
    
    # Confusion matrix for test data
    y_test_pred = np.argmax(model.predict(X_test), axis=1)
    test_cm = confusion_matrix(y_test_enc, y_test_pred)
    test_report = classification_report(y_test_enc, y_test_pred, target_names=label_encoder.classes_)
    
    plt.figure(figsize=(6, 5))
    sns.heatmap(test_cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix (Test Data)')
    plt.tight_layout()
    plt.show()
    
    print("Classification Report (Test Data):\n", test_report)
except Exception as e:
    print(f"Test set evaluation skipped: {e}")

    # Save the trained model to H5 file
model.save('3model4.h5')
print("Model saved successfully to action_recognition_model.h5")

# Optional: Save additional metadata if needed
import json

# Save model configuration details
model_config = {
    'categories': CATEGORIES,
    'frame_count': FRAME_COUNT,
    'frame_size': FRAME_SIZE,
    'epochs_trained': len(history.history['accuracy']),
    'final_training_accuracy': history.history['accuracy'][-1],
    'final_validation_accuracy': history.history['val_accuracy'][-1]
}

# Save metadata to a JSON file
with open('3model4.json', 'w') as f:
    json.dump(model_config, f, indent=4)

print("Model metadata saved to model_metadata.json")
