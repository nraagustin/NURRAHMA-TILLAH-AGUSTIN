# Import library 
import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import random
import json
import pickle

# Dataset path
DATASET_PATH = r"C:\Users\Lenovo\OneDrive\Documents\TA\DATASET"
CATEGORIES = ["BERDIRI", "DUDUK", "JATUH"]
FRAME_COUNT = 20
FRAME_SIZE = (64, 64)

# Load video function
def load_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while len(frames) < FRAME_COUNT:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, FRAME_SIZE)
        frame = frame / 255.0
        frames.append(frame)
    cap.release()

    # Pad with last frame if less than FRAME_COUNT
    while len(frames) < FRAME_COUNT and len(frames) > 0:
        frames.append(frames[-1])
    return np.array(frames)

# Load dataset function (simplified without balancing)
def load_dataset(split):
    X, y = [], []
    split_path = os.path.join(DATASET_PATH, split)
    for category in CATEGORIES:
        category_path = os.path.join(split_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: {category_path} tidak ditemukan.")
            continue

        for video_name in os.listdir(category_path):
            video_path = os.path.join(category_path, video_name)
            video_frames = load_video(video_path)
            if video_frames.shape[0] == FRAME_COUNT:
                X.append(video_frames)
                y.append(category)

    print(f"[INFO] Dataset {split}: {Counter(y)}")

    return np.array(X), np.array(y)

def apply_advanced_augmentation(X, y):
    X_aug, y_aug = [], []
    
    # Fungsi untuk transformasi warna
    def color_jitter(frame, brightness=0.2, contrast=0.2):
        # Brightness
        frame_bright = np.clip(frame * (1 + random.uniform(-brightness, brightness)), 0, 1)
        
        # Contrast enhancement
        frame_contrast = np.clip((frame_bright - 0.5) * (1 + random.uniform(-contrast, contrast)) + 0.5, 0, 1)
        
        return frame_contrast
    
    # Fungsi untuk spatial augmentation
    def spatial_augmentation(video):
        augmented_video = []
        for frame in video:
            # Random crop
            h, w = frame.shape[:2]
            new_h, new_w = int(h * random.uniform(0.8, 1.0)), int(w * random.uniform(0.8, 1.0))
            y_start = random.randint(0, h - new_h)
            x_start = random.randint(0, w - new_w)
            cropped_frame = frame[y_start:y_start+new_h, x_start:x_start+new_w]
            
            # Resize back to original size
            resized_frame = cv2.resize(cropped_frame, (w, h))
            augmented_video.append(resized_frame)
        
        return np.array(augmented_video)
    
    # Fungsi augmentasi temporal (baru)
    def temporal_augmentation(video):
        # Temporal jittering - memotong atau memperpanjang beberapa frame
        augmented_video = video.copy()
        
        # Random temporal cropping
        if random.random() > 0.5:
            start_frame = random.randint(0, int(len(video) * 0.3))
            end_frame = random.randint(int(len(video) * 0.7), len(video))
            augmented_video = augmented_video[start_frame:end_frame]
            
            # Pad atau resize kembali ke jumlah frame asli
            if len(augmented_video) < len(video):
                # Ulangi frame terakhir untuk mencapai jumlah frame semula
                padding = [augmented_video[-1]] * (len(video) - len(augmented_video))
                augmented_video = np.concatenate([augmented_video, padding])
        
        return augmented_video
    
    # Fungsi augmentasi noise (baru)
    def add_noise(frame, noise_type='gaussian'):
        row, col, ch = frame.shape
        
        if noise_type == 'gaussian':
            # Gaussian noise
            mean = 0
            var = 0.01
            sigma = var**0.5
            gauss = np.random.normal(mean, sigma, (row, col, ch))
            noisy = np.clip(frame + gauss, 0, 1)
        
        elif noise_type == 'salt_and_pepper':
            # Salt and pepper noise
            s_vs_p = 0.5
            amount = 0.004
            out = frame.copy()
            # Salt mode
            num_salt = np.ceil(amount * frame.size * s_vs_p)
            coords = [np.random.randint(0, i - 1, int(num_salt)) for i in frame.shape]
            out[coords[0], coords[1], coords[2]] = 1

            # Pepper mode
            num_pepper = np.ceil(amount* frame.size * (1. - s_vs_p))
            coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in frame.shape]
            out[coords[0], coords[1], coords[2]] = 0
            noisy = out
        
        return np.clip(noisy, 0, 1)
    
    # Proses augmentasi untuk setiap video
    for i in range(len(X)):
        video = X[i]
        label = y[i]
        
        # Tambahkan video asli
        X_aug.append(video)
        y_aug.append(label)
        
        # Variasi brightness
        brightness_factors = [0.6, 0.8, 1.2, 1.4]
        for factor in brightness_factors:
            augmented_video = np.clip(video * factor, 0, 1)
            X_aug.append(augmented_video)
            y_aug.append(label)
        
        # Rotasi dengan sudut berbeda
        rotation_angles = [-20, -10, 10, 20]
        for angle in rotation_angles:
            augmented_video = []
            for frame in video:
                rows, cols, _ = frame.shape
                M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
                rotated_frame = cv2.warpAffine(frame, M, (cols, rows))
                augmented_video.append(rotated_frame)
            X_aug.append(np.array(augmented_video))
            y_aug.append(label)
        
        # Horizontal dan vertikal shifts
        shift_ranges = [
            ((-0.1, 0.1), (-0.1, 0.1)),  # Larger shift range
            ((-0.05, 0.05), (-0.05, 0.05))  # Smaller shift range
        ]
        for (shift_x_min, shift_x_max), (shift_y_min, shift_y_max) in shift_ranges:
            shifted_x = random.uniform(shift_x_min, shift_x_max)
            shifted_y = random.uniform(shift_y_min, shift_y_max)
            M = np.float32([[1, 0, shifted_x*video.shape[2]], [0, 1, shifted_y*video.shape[1]]])
            
            shifted_video = []
            for frame in video:
                shifted_frame = cv2.warpAffine(frame, M, (frame.shape[1], frame.shape[0]))
                shifted_video.append(shifted_frame)
            
            X_aug.append(np.array(shifted_video))
            y_aug.append(label)
        
        # Augmentasi spasial
        if random.random() > 0.5:
            spatial_aug_video = spatial_augmentation(video)
            X_aug.append(spatial_aug_video)
            y_aug.append(label)
        
        # Temporal augmentation (TAMBAHAN BARU)
        if random.random() > 0.5:
            temporal_aug_video = temporal_augmentation(video)
            X_aug.append(temporal_aug_video)
            y_aug.append(label)
        
        # Color jittering
        if random.random() > 0.5:
            color_jittered_video = []
            for frame in video:
                jittered_frame = color_jitter(frame)
                color_jittered_video.append(jittered_frame)
            X_aug.append(np.array(color_jittered_video))
            y_aug.append(label)
        
        # Horizontal flip
        if random.random() > 0.5:
            flipped_video = np.flip(video, axis=2)
            X_aug.append(flipped_video)
            y_aug.append(label)
        
        # Tambahan augmentasi noise (TAMBAHAN BARU)
        noise_types = ['gaussian', 'salt_and_pepper']
        for noise_type in noise_types:
            if random.random() > 0.5:
                noisy_video = []
                for frame in video:
                    noisy_frame = add_noise(frame, noise_type)
                    noisy_video.append(noisy_frame)
                X_aug.append(np.array(noisy_video))
                y_aug.append(label)
        
        # Augmentasi khusus untuk kelas JATUH
        if label == "JATUH":
            # Kombinasi rotasi, brightness, dan spatial augmentation
            if random.random() > 0.5:
                complex_aug_video = []
                for frame in video:
                    # Rotasi
                    rows, cols, _ = frame.shape
                    angle = random.uniform(-15, 15)
                    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)
                    rotated_frame = cv2.warpAffine(frame, M, (cols, rows))
                    
                    # Brightness
                    brightness_factor = random.uniform(0.8, 1.2)
                    bright_frame = np.clip(rotated_frame * brightness_factor, 0, 1)
                    
                    # Spatial augmentation
                    h, w = bright_frame.shape[:2]
                    new_h, new_w = int(h * random.uniform(0.8, 1.0)), int(w * random.uniform(0.8, 1.0))
                    y_start = random.randint(0, h - new_h)
                    x_start = random.randint(0, w - new_w)
                    cropped_frame = bright_frame[y_start:y_start+new_h, x_start:x_start+new_w]
                    resized_frame = cv2.resize(cropped_frame, (w, h))
                    
                    complex_aug_video.append(resized_frame)
                
                X_aug.append(np.array(complex_aug_video))
                y_aug.append(label)
    
    return np.array(X_aug), np.array(y_aug)

# Tambahkan logging untuk melihat distribusi kelas setelah augmentasi
def log_class_distribution(y):
    class_counts = Counter(y)
    print("[INFO] Distribusi Kelas Setelah Augmentasi:")
    for category, count in class_counts.items():
        print(f"{category}: {count}")

def main():
    # Load training and validation datasets
    X_train, y_train = load_dataset('train')
    X_val, y_val = load_dataset('val')

    # Gunakan fungsi augmentasi yang baru
    X_train, y_train = apply_advanced_augmentation(X_train, y_train)
    log_class_distribution(y_train)
    print(f"[INFO] Dataset setelah augmentasi: {X_train.shape}")

    # Label encoding
    label_encoder = LabelEncoder()
    y_train_enc = label_encoder.fit_transform(y_train)
    y_val_enc = label_encoder.transform(y_val)

    # One-hot encode the labels for categorical_crossentropy
    y_train_onehot = tf.keras.utils.to_categorical(y_train_enc, num_classes=len(CATEGORIES))
    y_val_onehot = tf.keras.utils.to_categorical(y_val_enc, num_classes=len(CATEGORIES))

    # Model definition with the second block removed
    model = models.Sequential([
        # First convolutional block
        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same', 
                      input_shape=(FRAME_COUNT, FRAME_SIZE[0], FRAME_SIZE[1], 3),
                      kernel_regularizer=regularizers.l2(0.0005)),
        layers.BatchNormalization(),
        layers.MaxPooling3D((2, 2, 2)),
        layers.Dropout(0.3),

        # Third convolutional block (now becomes second block)
        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same',
                      kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.MaxPooling3D((2, 2, 2)),
        layers.Dropout(0.5),

        layers.Flatten(),
        layers.Dense(256, activation='relu', 
                    kernel_regularizer=regularizers.l2(0.002),
                    activity_regularizer=regularizers.l1(0.0003)),
        layers.Dropout(0.5),
        layers.Dense(len(CATEGORIES), activation='softmax')
    ])

    # Compile model
    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    model.summary()

    # Train model
    history = model.fit(
        X_train, y_train_onehot,
        validation_data=(X_val, y_val_onehot),
        epochs=50,
        batch_size=4,
        verbose=1
    )

    # Sisanya tetap sama seperti sebelumnya...

    # Plot akurasi & loss
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Akurasi Model')
    plt.xlabel('Epoch')
    plt.ylabel('Akurasi')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss Model')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # === Confusion Matrix ===
    # Prediksi pada data validasi
    y_pred_probs = model.predict(X_val)
    y_pred = np.argmax(y_pred_probs, axis=1)

    # Confusion matrix dan classification report
    cm = confusion_matrix(y_val_enc, y_pred)
    report = classification_report(y_val_enc, y_pred, target_names=label_encoder.classes_)

    # Plot confusion matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

    # Tampilkan classification report
    print("Classification Report:\n", report)

    # Save model in H5 format
    model.save('3model7.h5')
    print("3model7.h5")

    # Save label encoder classes for later use
    with open('3model7.pkl', 'wb') as f:
        pickle.dump(label_encoder.classes_, f)
    print("Label encoder classes saved as label_encoder_classes.pkl")

    # Evaluate on test set (if available)
    try:
        X_test, y_test = load_dataset('test')
        y_test_enc = label_encoder.transform(y_test)
        y_test_onehot = tf.keras.utils.to_categorical(y_test_enc, num_classes=len(CATEGORIES))
        
        test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)
        print(f"\nTest accuracy: {test_acc:.4f}")
        
        # Confusion matrix for test data
        y_test_pred_probs = model.predict(X_test)
        y_test_pred = np.argmax(y_test_pred_probs, axis=1)
        test_cm = confusion_matrix(y_test_enc, y_test_pred)
        test_report = classification_report(y_test_enc, y_test_pred, target_names=label_encoder.classes_)
        
        # Plot test confusion matrix
        plt.figure(figsize=(6, 5))
        sns.heatmap(test_cm, annot=True, fmt="d", cmap="Blues", 
                    xticklabels=label_encoder.classes_, 
                    yticklabels=label_encoder.classes_)
        plt.xlabel('Predicted Label')
        plt.ylabel('True Label')
        plt.title('Confusion Matrix (Test Data)')
        plt.tight_layout()
        plt.show()
        
        print("Classification Report (Test Data):\n", test_report)
    except Exception as e:
        print(f"Test set evaluation skipped: {e}")

    # Save additional model metadata
    model_config = {
        'categories': CATEGORIES,
        'frame_count': FRAME_COUNT,
        'frame_size': FRAME_SIZE,
        'epochs_trained': len(history.history['accuracy']),
        'final_training_accuracy': history.history['accuracy'][-1],
        'final_validation_accuracy': history.history['val_accuracy'][-1]
    }

    # Save metadata to a JSON file
    with open('3model7.json', 'w') as f:
        json.dump(model_config, f, indent=4)

    print("3model7.json")

# Run the main training script
if __name__ == '__main__':
    main()
